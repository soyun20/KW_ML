{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "tf.compat.v1.Session(config=config)\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# for i in range(len(gpus)): # gogo ÎÇòÎèÑ Í∑∏Í±∞ Ï∞æÏõÄ\n",
    "\t# tf.config.experimental.set_memory_growth(gpus[i], True)\n",
    "# Ïò§ÌÉÄ ÏàòÏ†ïÌï¥Ïç® ÎÅù„Öè„Öú ÎàåÎü¨Î∞î gpu -> gpus üëç Ï¥àÍ∏∞Ìôî ÌïòÍ≥† Îã§Ïãú~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Function to prepare training set and test set\n",
    "def prepare_data(filepath, src, dest):\n",
    "    classes_images = defaultdict(list)\n",
    "    with open(filepath, 'r') as txt:\n",
    "        paths = [read.strip() for read in txt.readlines()]\n",
    "        for p in paths:\n",
    "            food = p.split('/')\n",
    "            classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "    for food in classes_images.keys():\n",
    "        print(\"\\nCopying images into\", food)\n",
    "        if not os.path.exists(os.path.join(dest, food)):\n",
    "            os.makedirs(os.path.join(dest, food))\n",
    "        for i in classes_images[food]:\n",
    "            copy(os.path.join(src, food, i), os.path.join(dest, food, i))\n",
    "    print(\"Copying Done!\")\n",
    "\n",
    "# prepare_data('food-101/meta/train.txt', 'food-101/images', 'food-101/train')\n",
    "# prepare_data('food-101/meta/test.txt', 'food-101/images', 'food-101/test')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def identity_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters, kernel_size, padding='same')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization()(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def ResNet20(input_shape=(32, 32, 3), classes=101):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "\n",
    "    X = convolutional_block(X, 64, (3,3))\n",
    "    X = identity_block(X, 64, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 128, (3,3))\n",
    "    X = identity_block(X, 128, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 256, (3,3))\n",
    "    X = identity_block(X, 256, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 512, (3,3))\n",
    "    X = identity_block(X, 512, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet20')\n",
    "\n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "n_classes = 101\n",
    "img_width, img_height = 32, 32\n",
    "train_data_dir = 'food-101/train'\n",
    "validation_data_dir = 'food-101/test'\n",
    "nb_train_samples = 75750\n",
    "nb_validation_samples = 25250\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model = ResNet20(input_shape=(img_height, img_width, 3), classes=n_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_resnet20.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history_resnet20.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=nb_train_samples // batch_size,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=nb_validation_samples // batch_size,\n",
    "                              epochs=20,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_resnet20.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "from shutil import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to prepare training set and test set\n",
    "def prepare_data(filepath, src, dest):\n",
    "    classes_images = defaultdict(list)\n",
    "    with open(filepath, 'r') as txt:\n",
    "        paths = [read.strip() for read in txt.readlines()]\n",
    "        for p in paths:\n",
    "            food = p.split('/')\n",
    "            classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "    for food in classes_images.keys():\n",
    "        print(\"\\nCopying images into\", food)\n",
    "        if not os.path.exists(os.path.join(dest, food)):\n",
    "            os.makedirs(os.path.join(dest, food))\n",
    "        for i in classes_images[food]:\n",
    "            copy(os.path.join(src, food, i), os.path.join(dest, food, i))\n",
    "    print(\"Copying Done!\")\n",
    "\n",
    "# prepare_data('food-101/meta/train.txt', 'food-101/images', 'food-101/train')\n",
    "# prepare_data('food-101/meta/test.txt', 'food-101/images', 'food-101/test')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def identity_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, filters, kernel_size):\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters, kernel_size, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters, kernel_size, padding='same')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization()(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def ResNet20(input_shape=(299, 299, 3), classes=101):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "\n",
    "    X = convolutional_block(X, 64, (3,3))\n",
    "    X = identity_block(X, 64, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 128, (3,3))\n",
    "    X = identity_block(X, 128, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 256, (3,3))\n",
    "    X = identity_block(X, 256, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = convolutional_block(X, 512, (3,3))\n",
    "    X = identity_block(X, 512, (3,3))\n",
    "    X = MaxPooling2D(2, 2, padding='same')(X)\n",
    "\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet20')\n",
    "\n",
    "    return model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "n_classes = 101\n",
    "img_width, img_height = 299, 299\n",
    "train_data_dir = 'food-101/train'\n",
    "validation_data_dir = 'food-101/test'\n",
    "nb_train_samples = 75750\n",
    "nb_validation_samples = 25250\n",
    "batch_size = 4\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model = ResNet20(input_shape=(img_height, img_width, 3), classes=n_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_resnet20.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('history_resnet20.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=nb_train_samples // batch_size,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=nb_validation_samples // batch_size,\n",
    "                              epochs=10,\n",
    "                              verbose=1,\n",
    "                              callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_resnet20.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
